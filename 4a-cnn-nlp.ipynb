{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layers with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll see how we can use a convolutional layer to process and train on our NLP data.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating a Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so how can a convolutional layer be used to solve our NLP problem?  Well, remember that our process so far is to assign each token in our corpus relate to an index, and then have that index correspond to a word vector.  We were able to display a sequence of word vectors in a document with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./nlp-grid.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think back to what this shows.  Each row of numbers in the image above corresponds to a different word.  And the columns represent different indices of a word vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we were to apply a kernel to two entire rows, every column, it would be as if we were extracting the features from those two words.  This is how we can apply a kernel to text.  \n",
    "\n",
    "If we wish to analyze extract features from two words at a time, bigrams, we have a kernel that covers is 2 rows.  If we want to extract features from 3 words at a time, we cover three rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this point again, with a small example.  Imagine that we have a review that consists of the following four words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sentiment9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word is represented by a vector of five columns.  If we wish to extract features from bigrams, we then have a kernel size of $2x5$.  In applying our kernel to our embedded document, we apply the kernel to the document in sequence.\n",
    "\n",
    "> So the kernel is applied to words 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sentiment12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then to words 2 and 3. \n",
    "\n",
    "![](assets/sentiment13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on.\n",
    "\n",
    "![](assets/sentiment14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our resulting activation map will be a 3x1.  There are a sequence of 3 bigrams, with each application to a bigram resulting in single entry in the activation map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./activation-heat.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So for example, maybe the above extracts the amount of objectivity in a review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remember that we will apply multiple kernels to our text, with each kernel extracting different features from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./multiple-kernels.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now ultimately, we may wish to summarize the amount of these different features our document contains.  Here, if we wish to provide an average of the sentiment across the document, then we could use average pooling, with a dimension size equal to the length of the activation map.  Or perhaps it is the maximum of any one feature that is the best summary of the document.  In that case, we can use max pooling as displayed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/sentiment15.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model above, takes the text, translates it into word vectors, then we display one of the kernels extract different features from each bigram.  And finally, we summarize the resulting activation map with max pooling, telling us the maximum value for this feature across the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw how we can use convolutional layers to process text.  We do so, by first passing our text through an embedding, and then specifying a convolutional layer with each kernel spanning the length of the word vectors.  The number of rows in the kernel correspond to the number of words at a time to extract features from, that is, the length of the n-grams.  Finally, we can use max pooling to find the maximum feature value of a related document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
