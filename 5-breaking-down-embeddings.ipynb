{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Down Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The skipgram problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the skipgram model, we'll train a model that predicts for each word, what certain nearby words are.  And the goal is that by training a model to discover what words are nearby, the vector associated with a word will encapsulate information about what word is typically nearby, and thus information about the word itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving onto how we accomplish this task, let's make sure we have down our feature and target data.  Let's take a look at the last line in the diagram below.  We can see that the highlighted word is \"fox\".  This word will be our input feature.  We see that from the text, we have four observations where it is the input, and with `quick`, `brown`, `jumps` and `over` each taking a turn as the target.  Notice that we've now set up a classification problem, where given an input word we predict whether another word in our vocabulary is the specified word in the window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./skipgram.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this model is implemented using a shallow neural network.  Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use one hot encoding to retreive the word vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to represent each word in our corpus, our vocabulary, with  a one hot vector.  Then, we retreive a corresponding word embedding vector through matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w_{e} = x \\cdot W_e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vec_25 = np.eye(25000)[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "We = np.random.randn(25000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_e = vec_25.dot(We)\n",
    "w_e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64997304,  1.29472606,  0.80842406, -1.96071706,  0.82823269,\n",
       "       -0.86204791, -0.03670862, -0.59243377,  0.14133675, -0.3104924 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_e[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.64997304,  1.29472606,  0.80842406, -1.96071706,  0.82823269,\n",
       "       -0.86204791, -0.03670862, -0.59243377,  0.14133675, -0.3104924 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "We[25][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pass to output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a $w_e$ to represent every word in our vocabulary, and a mechanism to select each word, $x \\cdot W_e$, the next step is to predict whether any other word is in our window.  For this we need an output layer.  This output layer will consist of a linear layer and a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "output_layer = nn.Linear(300, 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4250, -0.6150, -0.0122,  ..., -0.6570, -0.7117, -0.5218],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "w_e_tensor = tensor(w_e).float()\n",
    "lin_outputs = output_layer(w_e_tensor)\n",
    "\n",
    "lin_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just pass these outputs through a softmax function, to turn these numbers into a prediction that any word in our vocabulary is within the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2347e-05, 1.8481e-05, 3.3767e-05,  ..., 1.7721e-05, 1.6777e-05,\n",
       "        2.0286e-05], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = F.softmax(lin_outputs, dim=0)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25000])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This embedding is used to predict the probability of a nearby word.  So this is the entire Word2vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\text{dog}$\n",
    "* $x_i = [0, 0, 0, 0, 1, 0, 0]_{25000}$\n",
    "* $e_{300} = E_{25000x300} \\cdot x_{i}$\n",
    "* $o_2 = e_{300} \\cdot O_{300x2500} $\n",
    "* $preds_{25000} = softmax(o_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skipgram model is kept purposely simple.  This is because the purpose isn't really to have a model that is as accurate as possible, but rather whose embeddings are as representative as possible.  Notice that the only items for us to tweak are the embedding layers and the output layer.  So this embedding layer has a strong influence on our predictions.\n",
    "\n",
    "Notice also that also because the performance of our model depends on how well the embedding layer predicts if a word is within the window, we are really defining our words in terms of it's surrounding words.  For example, if two different words have similar surrounding words, note that we would expect their embeddings to be fairly similar.\n",
    "\n",
    "One way of thinking about this assumption that we make in the skipgram model is that \"a word is defined by the company it keeps\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
