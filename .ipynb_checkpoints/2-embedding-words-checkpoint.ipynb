{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this lesson, we'll consider different ways that we can represent an NLP dataset so that it can be used by a machine learning algorithm.  So far, we have seen relatively simple ways for translating text into something numerical.  Essentially, we represented each word in our corpus by a different feature, and then indicated how important that word was to a document either by it's frequency (as in bag of words), or it's `term frequency` * `inverse document frequency`.\n",
    "\n",
    "With either our bag of words or `tfidf` representation, each word is a separate basis vector, and thus linearly independent of each of the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./car-nlp.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique has a couple of significant issues:\n",
    "\n",
    "1. Sparse vectors\n",
    "* The first is that with each additional word in our corpus, we need an additional feature.  More problematic is that some of these words will rarely occur, so it can become difficult to determine the significance of a word with relatively few training examples.\n",
    "\n",
    "2. Unrelated words\n",
    "\n",
    "* Another issue is that with each word represented as a separate dimension, we cannot tell the relationship between words. That is, we *do* think of words as being related to other words.  And even being combinations of other words, or qualities.\n",
    "\n",
    "In this lesson, we'll see how we can represent each *word* as a vector of attributes with word embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the idea of word embeddings is to represent words by some deeper underlying meaning.  For example, let's say we represent the words `king`, `queen`, and `president` with the following vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = [1, 1400, 2]\n",
    "queen = [0, 1400, 3]\n",
    "president = [.7, 1900, 1]\n",
    "prime_minister = [.6, 1900, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the first coordinate could represent the sex of associated with the word, a 1 for male and 0 for female.  The second coordinate represents the time period associated with the word, and the third represents the geography associated with the word (U.S. vs Europe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit with this representation of words is that our vectors are no longer completely independent of each other.  Instead, they have components of the other words.  So now, we can see that King and Queen appear more related than King and Prime Minister.\n",
    "\n",
    "In addition, we can now represent our corpus with fewer dimensions.  Previously, we essentially had a one hot encoding to represent each word, where every word is assigned an index.  With English having over 1 million words, this leads to a lot of dimensions.  With word embeddings, the number of dimensions ranges between 50 and 300 for each word.  And because our vectors are not perpendicular to one another, we can calculate how similar words are by calculating their closeness to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real World Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we can work with embeddings in Pytorch.  Pytorch makes available for different libraries of pretrained embeddings that allow us to represent our words with word embeddings.  Ok, let's once again download our IMDB dataset with torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:46<00:00, 1.83MB/s]\n",
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext import datasets, data\n",
    "import torch\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now this time when we build vocab we'll pass through an argument of `vectors = \"glove.6B.100d\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now downloading the glove library takes both time and a significant amount of space on a computer.  If you wish to delete the file look in the hidden `.vector_cache` folder, located in the your current folder.  If you wish to reuse the your local glove library, simply specify the name of the file to reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the way embeddings work is a bit of a two step.  Each word is assigned a separate index, just like before.  But then torchtext will use this index to find the 100 dimensional vector related to the word.  Let's see this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can again use string to integer to find the related index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then from there, we can use this index to find the related vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors[1124].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate how similar a word is to another word simply by looking at the distance between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7236])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puppy_vec = TEXT.vocab.vectors[TEXT.vocab.stoi['puppy']]\n",
    "dog_vec = TEXT.vocab.vectors[TEXT.vocab.stoi['dog']]\n",
    "\n",
    "torch.cosine_similarity(puppy_vec.unsqueeze(0), dog_vec.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_vec = TEXT.vocab.vectors[TEXT.vocab.stoi['banana']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2448])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(puppy_vec.unsqueeze(0), banana_vec.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to plot different words, we would see that words that we think of being more closely related are more closely related in our multidimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./word2vec-img.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we saw the benefits of using words vectors as opposed to one hot encoding to numericalize our text.  We saw that by using word vectors, we replace our sparse vectors with dense vectors.  Remember that sparse vectors are an issue, because in a sparse vector each feature oftentimes has a value of zero, so it's hard to determine it's impact.  And we saw that with word embeddings we are able to show the *relation* between words, which we could not do otherwise.\n",
    "\n",
    "Then, we saw how to use these embeddings with Pytorch's vocab object to map each word in our corpus to an index, which is then mapped to a related vector at that index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chris McCormick Embeddings](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "[Chris Olah Embeddings](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TorchText Embeddings U Toronto](https://www.cs.toronto.edu/~lczhang/360/lec/w06/w2v.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stanford - Intro to Embeddings](https://www.youtube.com/watch?v=8rXD5-xhemo&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[illustrated guide word2vec](http://jalammar.github.io/illustrated-word2vec/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Skipgram scratch](https://medium.com/district-data-labs/forward-propagation-building-a-skip-gram-net-from-the-ground-up-9578814b221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Skipgram again](https://iksinc.online/tag/skip-gram-model/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
