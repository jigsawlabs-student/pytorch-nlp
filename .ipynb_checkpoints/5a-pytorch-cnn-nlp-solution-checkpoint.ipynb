{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch CNN for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how we could apply the convolutional layers and pooling to an NLP problem.  The first step is translating the sequence of words in a document to corresponding word vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/sentiment9.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from there, applying kernels, that span the length of a word vector, and where the number of words to capture in sequence determine the number of rows of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/sentiment12.png\" width=\"32%\"> <img src=\"./assets/sentiment13.png\" width=\"32%\"> <img src=\"./assets/sentiment14.png\" width=\"32%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each kernel that we apply results in a different one dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./activation-heat.png\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can summarize the entire activation map across the document with average or max pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/sentiment15.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Before running any cells**, go to `Runtime`, and then `Change Runtime Type` to change colab to use GPU, and high memory if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's begin by loading up our data our from IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know we need to initialize a Field and LabelField to perform some initial preprocessing upon download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize `TEXT` with the spacy tokenizer.  Set **batch_first** to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy', batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function _spacy_tokenize at 0x128a00a70>, spacy=<spacy.lang.en.English object at 0x12b57ecd0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.tokenize\n",
    "# functools.partial(<function _spacy_tokenize at 0x128a00a70>, spacy=<spacy.lang.en.English object at 0x12b57ecd0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the `LABEL` field and set the datatype as float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.dtype\n",
    "\n",
    "# torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then download the IMDB dataset, passing through the `TEXT` and `LABEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can numericalize our data, by calling the `build_vocab` method to associate each word in our corpus with both an index, and a related vector.  If a word is not found with the pretrained glove representation, represented with a random vector, as specified in `unk_init`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [13:28, 1.07MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:24<00:00, 16462.77it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# device = torch.device('cuda')\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = 25000, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call `build_vocab` on `LABEL` to convert the labels into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use our BucketIterator to batch our data.\n",
    "\n",
    "> In doing so, pass through a keyword argument of `device` and pass through an initialized `device = torch.device('cuda')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size = 64, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at a batch of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    text = batch.text\n",
    "    labels = batch.label\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can access our text and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 725])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's begin to build our neural network in Pytorch.  The first step is to initialize our embedding.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the  **Embedding**\n",
    "* As we know, the number of embeddings should equal the number of words in our vocabulary.  And the `embedding_dim` should equal the number of columns for each word vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First use the vocab object to find the number of words in the vocabulary.  Assign it to `num_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = len(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now define neural network with only an embedding.  The `forward` method should take in numericalized and return the corresponding word vectors from our randomly initialized embedding.\n",
    "\n",
    "> We can move add in the wordvectors from Glove later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(25002, \n",
    "                                      100)\n",
    "    def forward(self, text):        \n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(25002, 100)\n",
       ")"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net\n",
    "\n",
    "# Net(\n",
    "#   (embedding): Embedding(25002, 100, padding_idx=1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this out with our first batch of input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 725, 100])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(text).shape\n",
    "\n",
    "# torch.Size([64, 725, 100])\n",
    "# middle number may be different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we can see that for our batch of 64 observations, a certain number of words per observation, and with each word being represented by a vector of length 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next up is to define our first convolutional layer.  \n",
    "\n",
    "For the first convolutional layer, the number of channels should be one.  We can have 100 different filters.  And the kernel size should be equal to the length of the word vector.  Write the layer so that the kernel is applied to three words at a time.  Let's assign the convolution as `conv_0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 1146, 100])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(text).shape\n",
    "\n",
    "# torch.Size([64, 1146, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copy the code from the earlier neural network to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(25002, \n",
    "                                      100)\n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = 100, \n",
    "                                kernel_size = (5, 100))\n",
    "    def forward(self, text):        \n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        embedded_with_channel = embedded.unsqueeze(1)\n",
    "        conved_0 = F.relu(self.conv_0(embedded_with_channel).squeeze(3))\n",
    "        #conved_0 = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        #conved_0 = [64, 100, 1142]\n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[-1]).squeeze(2)\n",
    "        return pooled_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(text).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's fill in the forward method, on the code above.  \n",
    "\n",
    "* First, after the code is returned from the embedding, we'll have to add a dimension for the channel.  This way, before being passed to a convolution, the data is in the shape of `torch.Size([batch size, channel, document length, word vectors])`.  The channel size is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then apply the sequence of `conv > relu > maxpool1d`.\n",
    "    * Before maxpooling the shape will be `[64, 100, document size - 3 + 1]`\n",
    "    * We should maxpool across the entire length of the vector.\n",
    "    * After maxpooling, shape will be `[64, 100]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check the shape of your data, returning the output from max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_max_pool = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 100])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_max_pool(text).shape\n",
    "\n",
    "# torch.Size([64, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now for each document, we have a single summary number from each filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding more Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep going with our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we have a single convolutional layer we extract the features from a sequence of five words in a review at a time.  We can add 2 word and 3 word sequences to the mix by adding another convolutional layer for each and also feeding passing them the outputs from the embedded layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cnndiag.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then concatenate all of these outputs together, and pass the concatenated vectors (one for each observation) to a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll have two additional convolutional layers, assigned to `conv_1`, and `conv_2`.  One that takes in n-grams of length 4, the other of length 5, and with the sequences of: \n",
    "\n",
    "* `embedding > conv_2 > relu > max pool`\n",
    "* `embedding > conv_3 > relu > max pool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing the data to the linear layer concatenate the outputs from the convolutional layers together with:\n",
    "\n",
    "```python\n",
    "concat_pooled = torch.cat((pooled_0, pooled_1, pooled_2), dim = 1)\n",
    "```\n",
    "\n",
    "Finally, let's finish up with a linear layer that takes in `3*100` inputs.  One input for each of the output channels in each of the convolutional layers.  And has a single output of a number between 1 and 0.  \n",
    "> Do not worry about a final activation layer.  That can be taken care with the cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can add a dropout of that we apply before passing our data to the linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(25002, \n",
    "                                      100,\n",
    "                              padding_idx = 1)\n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = 100, \n",
    "                                kernel_size = (3, 100))\n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = 100, \n",
    "                                kernel_size = (4, 100))\n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = 100, \n",
    "                                kernel_size = (5, 100))\n",
    "        \n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.linear = nn.Linear(3*100, 1)\n",
    "        \n",
    "    def forward(self, text):        \n",
    "        #text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        embedded_with_channel = embedded.unsqueeze(1)\n",
    "        conved_0 = F.relu(self.conv_0(embedded_with_channel).squeeze(3))\n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[-1]).squeeze(2)\n",
    "        \n",
    "        conved_1 = F.relu(self.conv_1(embedded_with_channel).squeeze(3))\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[-1]).squeeze(2)\n",
    "        #conved_0 = [64, 50]\n",
    "        conved_2 = F.relu(self.conv_2(embedded_with_channel).squeeze(3))\n",
    "        pooled_2 = F.max_pool1d(conved_1, conved_1.shape[-1]).squeeze(2)\n",
    "        concat_pooled = torch.cat((pooled_0, pooled_1, pooled_2), dim = 1)\n",
    "        dropout = self.dropout(concat_pooled)\n",
    "        L1 = self.linear(dropout)\n",
    "        return L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the neural network structure matches what's specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiConvNet(\n",
       "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
       "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
       "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
       "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_conv = MultiConvNet()\n",
    "multi_conv\n",
    "\n",
    "# MultiConvNet(\n",
    "#   (embedding): Embedding(25002, 100, padding_idx=1)\n",
    "#   (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
    "#   (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
    "#   (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
    "#   (dropout): Dropout(p=0.5, inplace=False)\n",
    "#   (linear): Linear(in_features=300, out_features=1, bias=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the calculations in the `multi_conv` net on cuda, run the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_conv = multi_conv.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's move over our pretrained embeddings to the neural network.  Get the pretrained embeddings from the vocab object on our `TEXT` field.  Assign them to `pretrained_embeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25002, 100])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings.shape\n",
    "# torch.Size([25002, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then `copy` the embeddings to the `multiconv`'s embedding layer's `weight.data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3092, -0.4611,  0.3395,  ..., -0.2275, -0.7580, -0.2616],\n",
       "        [ 0.2219, -1.2999,  0.4127,  ...,  1.4192, -0.3857,  1.1795],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.3177, -0.4069, -0.4231,  ...,  0.2272, -0.6000,  0.3436],\n",
       "        [ 0.6385,  1.1301,  0.1818,  ...,  0.4541, -0.7086, -0.4312],\n",
       "        [ 0.6339,  0.0847, -1.0509,  ...,  0.2354, -1.3884, -0.3012]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_conv.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll zero the initial weights of the unknown and padding tokens.  To do so, first get the `pad_token` and `unk_token` from the `vocab` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_token = TEXT.unk_token\n",
    "unknown_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_token = TEXT.pad_token\n",
    "padding_token\n",
    "\n",
    "# '<pad>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these are both the strings.  So now we should be able to find the corresponding index of the padding and the unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "unknown_idx\n",
    "\n",
    "# 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "pad_idx\n",
    "\n",
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select the vectors at those indices in the embedding and replace them with a vector of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_conv.embedding.weight.data[unknown_index] = torch.zeros(100)\n",
    "multi_conv.embedding.weight.data[pad_idx] = torch.zeros(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to move onto training the model.  First let's define our optimizer by passing through the parameters and setting a learning rate of `.0005`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(multi_conv.parameters(), lr = .0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0005\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer\n",
    "\n",
    "# Adam (\n",
    "# Parameter Group 0\n",
    "#     amsgrad: False\n",
    "#     betas: (0.9, 0.999)\n",
    "#     eps: 1e-08\n",
    "#     lr: 0.0005\n",
    "#     weight_decay: 0\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is to define the loss function.  For the loss function initialize an instance of `BCEWithLogitsLoss`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll have both the neural network and the loss function run on the cuda device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "bce_loss = bce_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BCEWithLogitsLoss stands for binary cross entropy with logits loss.  Now binary cross entropy we've seen before.  It's simply cross entropy when the output is either 1 or 0.  In other words, it's another word for log loss.  Remember that for log loss we reward a hypothesis function for predicting a number close to 0 when the true label is a zero, and a 1 when the true label is a 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.605170185988091, 0.01005033585350145)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# When 1, loss is -log(p) \n",
    "-np.log(.01), -np.log(1 - .01)\n",
    "# When 0, loss is -log(1 - p) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So that takes care of the BCE part, the \"with logits\" is because of the output of our neural network.  Remember, we did not pass the output through an activation function (in the binary case it would be the sigmoid function).  The withlogits corresponds to that.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the loss function, we'll pass through a batch of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = multi_conv(batch.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2708],\n",
       "        [-0.3760]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 1.])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But we need to remove the individual rows from the predictions, so they are of the same shape as the labels.  So we squeeze the predictions and then pass them, and the label into our `bce_loss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = bce_loss(predictions.squeeze(1), batch.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's move through our training loop.  We can go through six epochs, and see how we do.  \n",
    "\n",
    "> This may take a while, so feel free to move on to the next lesson after things are running properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff/opt/anaconda3/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4136, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.3570, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.2350, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.1163, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.1894, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
      "tensor(0.0493, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(6):\n",
    "    for batch in train_iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = multi_conv(batch.text)\n",
    "        \n",
    "        loss = bce_loss(predictions.squeeze(1), batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this is complete, let's move onto testing our model.  To do this, we can move through the batches of our model, calculating the score and the number of samples in the batch.  This way we can compute a weighted average at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(2))\n",
    "class_total = list(0. for i in range(2))\n",
    "with torch.no_grad():\n",
    "    for batch in test_iterator:\n",
    "        outputs = multi_conv(batch.text)\n",
    "        labels = batch.label\n",
    "        hard_outputs = torch.round(torch.sigmoid(outputs.reshape(-1))).int()\n",
    "        is_corrects = (labels == hard_outputs).int()\n",
    "        for label, is_correct in zip(labels, is_corrects):\n",
    "            label_int = label.int().item()\n",
    "            class_correct[label_int] += is_correct.item()\n",
    "            class_total[label_int] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use the following function to try out how our model handles some random text.  The function tokenizes a sentence, and then has our model predict the sentiment.  Try it out below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example negative review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5142894387245178"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(multi_conv, \"not good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example positive review..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988067150115967"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(multi_conv, \"it's good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Lab based excellent material of [Bentrevett](https://github.com/bentrevett/pytorch-sentiment-analysis)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
